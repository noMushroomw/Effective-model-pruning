{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7710649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.models import resnet50, resnet18, resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c5f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_loaders(dataset_name, batch_size=128, test_batch_size=1000, data_root='./data'):\n",
    "    \"\"\"\n",
    "    Returns: train_loader, test_loader, input_size, num_classes, meta (dict)\n",
    "    \"\"\"\n",
    "    name = dataset_name.lower()\n",
    "    meta = {}\n",
    "\n",
    "    # Generic normalizations (safe defaults). If you want canonical stats, compute them once.\n",
    "    NORM_1C = transforms.Normalize((0.5,), (0.5,))\n",
    "    NORM_3C = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    if name == 'mnist':\n",
    "        # (You already have this; included for completeness.)\n",
    "        tfm = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train = datasets.MNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.MNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'fashionmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.FashionMNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.FashionMNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "        \n",
    "    elif name == 'cifar10':\n",
    "        tfm = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                NORM_3C\n",
    "            ])\n",
    "        train = datasets.CIFAR10(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR10(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 224*224*3, 10\n",
    "\n",
    "    elif name == 'cifar100':\n",
    "        tfm = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                NORM_3C\n",
    "            ])\n",
    "        train = datasets.CIFAR100(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR100(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 224*224*3, 100\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=test_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, test_loader, inp, ncls, meta\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test(model, device, test_loader, criterion, times=1):\n",
    "    model.eval()\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for _ in range(times):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy)\n",
    "        loss_list.append(test_loss)\n",
    "    if times == 1:\n",
    "        return test_loss, accuracy\n",
    "    else:\n",
    "        return loss_list, accuracy_list, sum(accuracy_list) / times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name = 'cifar10'\n",
    "train_loader, test_loader, input_size, num_classes, meta = get_loaders(datasets_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cifar_10_model_configs = {\n",
    "    'resnet18': {\n",
    "        'model': resnet18,\n",
    "        'pretrained': False,\n",
    "        'input_size': input_size,\n",
    "        'num_classes': num_classes,\n",
    "        'lr': 0.01,\n",
    "        'epochs': 10\n",
    "    },\n",
    "    'resnet50': {\n",
    "        'model': resnet50,\n",
    "        'pretrained': False,\n",
    "        'input_size': input_size,\n",
    "        'num_classes': num_classes,\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 20,\n",
    "    },\n",
    "    'resnet101': {\n",
    "        'model': resnet101,\n",
    "        'pretrained': False,\n",
    "        'input_size': input_size,\n",
    "        'num_classes': num_classes,\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 40,\n",
    "    }\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba02b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: resnet18, Epoch: 1, Train Loss: 1.3475, Train Acc: 50.87, Test Loss: 0.0012, Test Acc: 57.43\n",
      "Model: resnet18, Epoch: 2, Train Loss: 0.8452, Train Acc: 69.67, Test Loss: 0.0008, Test Acc: 71.87\n",
      "Model: resnet18, Epoch: 3, Train Loss: 0.6285, Train Acc: 77.88, Test Loss: 0.0007, Test Acc: 76.46\n",
      "Model: resnet18, Epoch: 4, Train Loss: 0.4978, Train Acc: 82.63, Test Loss: 0.0008, Test Acc: 72.92\n",
      "Model: resnet18, Epoch: 5, Train Loss: 0.4060, Train Acc: 85.86, Test Loss: 0.0006, Test Acc: 79.86\n",
      "Model: resnet18, Epoch: 6, Train Loss: 0.3159, Train Acc: 88.77, Test Loss: 0.0005, Test Acc: 82.69\n",
      "Model: resnet18, Epoch: 7, Train Loss: 0.2430, Train Acc: 91.56, Test Loss: 0.0008, Test Acc: 76.91\n",
      "Model: resnet18, Epoch: 8, Train Loss: 0.1775, Train Acc: 93.66, Test Loss: 0.0006, Test Acc: 80.82\n",
      "Model: resnet18, Epoch: 9, Train Loss: 0.1293, Train Acc: 95.36, Test Loss: 0.0006, Test Acc: 82.66\n",
      "Model: resnet18, Epoch: 10, Train Loss: 0.0931, Train Acc: 96.67, Test Loss: 0.0007, Test Acc: 82.19\n",
      "Model: resnet50, Epoch: 1, Train Loss: 1.5559, Train Acc: 43.16, Test Loss: 0.0012, Test Acc: 55.78\n",
      "Model: resnet50, Epoch: 2, Train Loss: 0.9967, Train Acc: 64.12, Test Loss: 0.0012, Test Acc: 61.24\n",
      "Model: resnet50, Epoch: 3, Train Loss: 0.7617, Train Acc: 73.20, Test Loss: 0.0007, Test Acc: 74.90\n",
      "Model: resnet50, Epoch: 4, Train Loss: 0.6039, Train Acc: 78.78, Test Loss: 0.0007, Test Acc: 75.84\n",
      "Model: resnet50, Epoch: 5, Train Loss: 0.5019, Train Acc: 82.51, Test Loss: 0.0007, Test Acc: 76.75\n",
      "Model: resnet50, Epoch: 6, Train Loss: 0.4250, Train Acc: 85.23, Test Loss: 0.0006, Test Acc: 79.99\n",
      "Model: resnet50, Epoch: 7, Train Loss: 0.3596, Train Acc: 87.35, Test Loss: 0.0006, Test Acc: 80.62\n",
      "Model: resnet50, Epoch: 8, Train Loss: 0.2862, Train Acc: 90.07, Test Loss: 0.0006, Test Acc: 80.40\n",
      "Model: resnet50, Epoch: 9, Train Loss: 0.2375, Train Acc: 91.63, Test Loss: 0.0006, Test Acc: 80.86\n",
      "Model: resnet50, Epoch: 10, Train Loss: 0.1850, Train Acc: 93.44, Test Loss: 0.0007, Test Acc: 80.38\n",
      "Model: resnet101, Epoch: 1, Train Loss: 1.6181, Train Acc: 40.53, Test Loss: 0.0013, Test Acc: 51.69\n",
      "Model: resnet101, Epoch: 2, Train Loss: 1.0864, Train Acc: 60.80, Test Loss: 0.0014, Test Acc: 59.07\n",
      "Model: resnet101, Epoch: 3, Train Loss: 0.8161, Train Acc: 71.07, Test Loss: 0.0009, Test Acc: 67.75\n",
      "Model: resnet101, Epoch: 4, Train Loss: 0.6368, Train Acc: 77.71, Test Loss: 0.0008, Test Acc: 73.85\n"
     ]
    }
   ],
   "source": [
    "for model_name, config in cifar_10_model_configs.items():\n",
    "    model = config['model'](pretrained=config['pretrained'], num_classes=config['num_classes']).to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(1, 11):\n",
    "        train_loss, train_acc = train(model, device, train_loader, optimizer, criterion)\n",
    "        test_loss, test_acc = test(model, device, test_loader, criterion)\n",
    "        print(f\"Model: {model_name}, Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}\")\n",
    "        \n",
    "    model_path = f\"./models/{datasets_name}/{model_name}.pth\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748dad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
