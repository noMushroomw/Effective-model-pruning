{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7710649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.models import resnet50, resnet18, resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c5f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_loaders(dataset_name, batch_size=128, test_batch_size=1000, data_root='./data'):\n",
    "    \"\"\"\n",
    "    Returns: train_loader, test_loader, input_size, num_classes, meta (dict)\n",
    "    \"\"\"\n",
    "    name = dataset_name.lower()\n",
    "    meta = {}\n",
    "\n",
    "    # Generic normalizations (safe defaults). If you want canonical stats, compute them once.\n",
    "    NORM_1C = transforms.Normalize((0.5,), (0.5,))\n",
    "    NORM_3C = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    if name == 'mnist':\n",
    "        # (You already have this; included for completeness.)\n",
    "        tfm = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train = datasets.MNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.MNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'fashionmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.FashionMNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.FashionMNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "        \n",
    "    elif name == 'cifar10':\n",
    "        tfm = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                NORM_3C\n",
    "            ])\n",
    "        train = datasets.CIFAR10(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR10(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 224*224*3, 10\n",
    "\n",
    "    elif name == 'cifar100':\n",
    "        tfm = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                NORM_3C\n",
    "            ])\n",
    "        train = datasets.CIFAR100(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR100(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 224*224*3, 100\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=test_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, test_loader, inp, ncls, meta\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test(model, device, test_loader, criterion, times=1):\n",
    "    model.eval()\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for _ in range(times):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy)\n",
    "        loss_list.append(test_loss)\n",
    "    if times == 1:\n",
    "        return test_loss, accuracy\n",
    "    else:\n",
    "        return loss_list, accuracy_list, sum(accuracy_list) / times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca02f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name = 'cifar10'\n",
    "train_loader, test_loader, input_size, num_classes, meta = get_loaders(datasets_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cifar_10_model_configs = {\n",
    "    'resnet18': {\n",
    "        'model': resnet18,\n",
    "        'pretrained': False,\n",
    "        'input_size': input_size,\n",
    "        'num_classes': num_classes,\n",
    "        'lr': 0.01,\n",
    "        'epochs': 10\n",
    "    },\n",
    "    'resnet50': {\n",
    "        'model': resnet50,\n",
    "        'pretrained': False,\n",
    "        'input_size': input_size,\n",
    "        'num_classes': num_classes,\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 20,\n",
    "    },\n",
    "    'resnet101': {\n",
    "        'model': resnet101,\n",
    "        'pretrained': False,\n",
    "        'input_size': input_size,\n",
    "        'num_classes': num_classes,\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 40,\n",
    "    }\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba02b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: resnet18, Epoch: 1, Train Loss: 1.3452, Train Acc: 50.77, Test Loss: 0.0012, Test Acc: 58.70\n",
      "Model: resnet18, Epoch: 2, Train Loss: 0.8120, Train Acc: 71.27, Test Loss: 0.0008, Test Acc: 73.11\n",
      "Model: resnet18, Epoch: 3, Train Loss: 0.6069, Train Acc: 78.64, Test Loss: 0.0009, Test Acc: 72.68\n",
      "Model: resnet18, Epoch: 4, Train Loss: 0.4740, Train Acc: 83.53, Test Loss: 0.0007, Test Acc: 76.88\n",
      "Model: resnet18, Epoch: 5, Train Loss: 0.3797, Train Acc: 86.74, Test Loss: 0.0007, Test Acc: 78.60\n",
      "Model: resnet18, Epoch: 6, Train Loss: 0.2926, Train Acc: 89.75, Test Loss: 0.0006, Test Acc: 78.75\n",
      "Model: resnet18, Epoch: 7, Train Loss: 0.2165, Train Acc: 92.51, Test Loss: 0.0007, Test Acc: 80.76\n",
      "Model: resnet18, Epoch: 8, Train Loss: 0.1561, Train Acc: 94.46, Test Loss: 0.0006, Test Acc: 82.07\n",
      "Model: resnet18, Epoch: 9, Train Loss: 0.1081, Train Acc: 96.24, Test Loss: 0.0006, Test Acc: 82.80\n",
      "Model: resnet18, Epoch: 10, Train Loss: 0.0863, Train Acc: 96.93, Test Loss: 0.0006, Test Acc: 83.85\n",
      "Model: resnet50, Epoch: 1, Train Loss: 1.5463, Train Acc: 43.96, Test Loss: 0.0012, Test Acc: 57.50\n",
      "Model: resnet50, Epoch: 2, Train Loss: 1.0007, Train Acc: 64.23, Test Loss: 0.0010, Test Acc: 66.57\n",
      "Model: resnet50, Epoch: 3, Train Loss: 0.7787, Train Acc: 72.40, Test Loss: 0.0008, Test Acc: 71.07\n",
      "Model: resnet50, Epoch: 4, Train Loss: 0.6199, Train Acc: 78.38, Test Loss: 0.0008, Test Acc: 72.48\n",
      "Model: resnet50, Epoch: 5, Train Loss: 0.5241, Train Acc: 81.78, Test Loss: 0.0007, Test Acc: 78.66\n",
      "Model: resnet50, Epoch: 6, Train Loss: 0.4265, Train Acc: 84.97, Test Loss: 0.0005, Test Acc: 81.75\n",
      "Model: resnet50, Epoch: 7, Train Loss: 0.3517, Train Acc: 87.56, Test Loss: 0.0006, Test Acc: 81.28\n",
      "Model: resnet50, Epoch: 8, Train Loss: 0.2873, Train Acc: 89.95, Test Loss: 0.0006, Test Acc: 79.48\n",
      "Model: resnet50, Epoch: 9, Train Loss: 0.2371, Train Acc: 91.64, Test Loss: 0.0005, Test Acc: 83.48\n",
      "Model: resnet50, Epoch: 10, Train Loss: 0.1867, Train Acc: 93.40, Test Loss: 0.0006, Test Acc: 82.10\n",
      "Model: resnet50, Epoch: 11, Train Loss: 0.1508, Train Acc: 94.64, Test Loss: 0.0006, Test Acc: 83.08\n",
      "Model: resnet50, Epoch: 12, Train Loss: 0.1233, Train Acc: 95.61, Test Loss: 0.0007, Test Acc: 81.69\n",
      "Model: resnet50, Epoch: 13, Train Loss: 0.1035, Train Acc: 96.30, Test Loss: 0.0007, Test Acc: 83.01\n",
      "Model: resnet50, Epoch: 14, Train Loss: 0.0881, Train Acc: 96.95, Test Loss: 0.0008, Test Acc: 81.39\n",
      "Model: resnet50, Epoch: 15, Train Loss: 0.0857, Train Acc: 97.02, Test Loss: 0.0007, Test Acc: 83.79\n",
      "Model: resnet50, Epoch: 16, Train Loss: 0.0666, Train Acc: 97.67, Test Loss: 0.0007, Test Acc: 83.11\n",
      "Model: resnet50, Epoch: 17, Train Loss: 0.0711, Train Acc: 97.55, Test Loss: 0.0007, Test Acc: 83.71\n",
      "Model: resnet50, Epoch: 18, Train Loss: 0.0571, Train Acc: 98.04, Test Loss: 0.0008, Test Acc: 82.86\n",
      "Model: resnet50, Epoch: 19, Train Loss: 0.0590, Train Acc: 97.92, Test Loss: 0.0008, Test Acc: 83.30\n",
      "Model: resnet50, Epoch: 20, Train Loss: 0.0495, Train Acc: 98.25, Test Loss: 0.0008, Test Acc: 83.77\n",
      "Model: resnet101, Epoch: 1, Train Loss: 1.6403, Train Acc: 39.83, Test Loss: 0.0015, Test Acc: 45.84\n",
      "Model: resnet101, Epoch: 2, Train Loss: 1.0992, Train Acc: 60.51, Test Loss: 0.0011, Test Acc: 64.27\n",
      "Model: resnet101, Epoch: 3, Train Loss: 0.8045, Train Acc: 71.64, Test Loss: 0.0009, Test Acc: 69.07\n",
      "Model: resnet101, Epoch: 4, Train Loss: 0.6310, Train Acc: 78.14, Test Loss: 0.0007, Test Acc: 76.66\n",
      "Model: resnet101, Epoch: 5, Train Loss: 0.5181, Train Acc: 81.97, Test Loss: 0.0006, Test Acc: 79.94\n",
      "Model: resnet101, Epoch: 6, Train Loss: 0.4377, Train Acc: 84.76, Test Loss: 0.0007, Test Acc: 76.65\n",
      "Model: resnet101, Epoch: 7, Train Loss: 0.3609, Train Acc: 87.53, Test Loss: 0.0006, Test Acc: 80.72\n",
      "Model: resnet101, Epoch: 8, Train Loss: 0.3112, Train Acc: 89.15, Test Loss: 0.0006, Test Acc: 81.39\n",
      "Model: resnet101, Epoch: 9, Train Loss: 0.2510, Train Acc: 91.14, Test Loss: 0.0005, Test Acc: 83.12\n",
      "Model: resnet101, Epoch: 10, Train Loss: 0.2104, Train Acc: 92.55, Test Loss: 0.0006, Test Acc: 81.37\n",
      "Model: resnet101, Epoch: 11, Train Loss: 0.1684, Train Acc: 94.12, Test Loss: 0.0006, Test Acc: 82.76\n",
      "Model: resnet101, Epoch: 12, Train Loss: 0.1398, Train Acc: 94.95, Test Loss: 0.0008, Test Acc: 80.96\n",
      "Model: resnet101, Epoch: 13, Train Loss: 0.1156, Train Acc: 95.91, Test Loss: 0.0006, Test Acc: 83.04\n",
      "Model: resnet101, Epoch: 14, Train Loss: 0.1014, Train Acc: 96.30, Test Loss: 0.0007, Test Acc: 82.54\n",
      "Model: resnet101, Epoch: 15, Train Loss: 0.0898, Train Acc: 96.84, Test Loss: 0.0008, Test Acc: 81.78\n",
      "Model: resnet101, Epoch: 16, Train Loss: 0.0733, Train Acc: 97.41, Test Loss: 0.0006, Test Acc: 84.26\n",
      "Model: resnet101, Epoch: 17, Train Loss: 0.0800, Train Acc: 97.35, Test Loss: 0.0022, Test Acc: 74.02\n",
      "Model: resnet101, Epoch: 18, Train Loss: 0.1011, Train Acc: 96.53, Test Loss: 0.0006, Test Acc: 84.68\n",
      "Model: resnet101, Epoch: 19, Train Loss: 0.0508, Train Acc: 98.27, Test Loss: 0.0007, Test Acc: 83.56\n",
      "Model: resnet101, Epoch: 20, Train Loss: 0.0518, Train Acc: 98.21, Test Loss: 0.0007, Test Acc: 84.85\n",
      "Model: resnet101, Epoch: 21, Train Loss: 0.0495, Train Acc: 98.24, Test Loss: 0.0007, Test Acc: 84.63\n",
      "Model: resnet101, Epoch: 22, Train Loss: 0.0351, Train Acc: 98.79, Test Loss: 0.0007, Test Acc: 84.69\n",
      "Model: resnet101, Epoch: 23, Train Loss: 0.0414, Train Acc: 98.63, Test Loss: 0.0008, Test Acc: 83.70\n",
      "Model: resnet101, Epoch: 24, Train Loss: 0.0478, Train Acc: 98.33, Test Loss: 0.0007, Test Acc: 84.18\n",
      "Model: resnet101, Epoch: 25, Train Loss: 0.0357, Train Acc: 98.79, Test Loss: 0.0008, Test Acc: 83.51\n",
      "Model: resnet101, Epoch: 26, Train Loss: 0.0457, Train Acc: 98.41, Test Loss: 0.0007, Test Acc: 85.48\n",
      "Model: resnet101, Epoch: 27, Train Loss: 0.0354, Train Acc: 98.80, Test Loss: 0.0007, Test Acc: 85.31\n",
      "Model: resnet101, Epoch: 28, Train Loss: 0.0286, Train Acc: 98.98, Test Loss: 0.0008, Test Acc: 83.57\n",
      "Model: resnet101, Epoch: 29, Train Loss: 0.0340, Train Acc: 98.85, Test Loss: 0.0007, Test Acc: 85.42\n",
      "Model: resnet101, Epoch: 30, Train Loss: 0.0257, Train Acc: 99.13, Test Loss: 0.0008, Test Acc: 84.74\n",
      "Model: resnet101, Epoch: 31, Train Loss: 0.0305, Train Acc: 98.97, Test Loss: 0.0008, Test Acc: 84.15\n",
      "Model: resnet101, Epoch: 32, Train Loss: 0.0391, Train Acc: 98.67, Test Loss: 0.0009, Test Acc: 82.46\n",
      "Model: resnet101, Epoch: 33, Train Loss: 0.0246, Train Acc: 99.13, Test Loss: 0.0008, Test Acc: 85.67\n",
      "Model: resnet101, Epoch: 34, Train Loss: 0.0254, Train Acc: 99.13, Test Loss: 0.0009, Test Acc: 84.00\n",
      "Model: resnet101, Epoch: 35, Train Loss: 0.0271, Train Acc: 99.03, Test Loss: 0.0008, Test Acc: 84.60\n",
      "Model: resnet101, Epoch: 36, Train Loss: 0.0227, Train Acc: 99.27, Test Loss: 0.0008, Test Acc: 84.41\n",
      "Model: resnet101, Epoch: 37, Train Loss: 0.0276, Train Acc: 99.12, Test Loss: 0.0008, Test Acc: 84.58\n",
      "Model: resnet101, Epoch: 38, Train Loss: 0.0287, Train Acc: 98.99, Test Loss: 0.0009, Test Acc: 84.43\n",
      "Model: resnet101, Epoch: 39, Train Loss: 0.0214, Train Acc: 99.30, Test Loss: 0.0008, Test Acc: 84.46\n",
      "Model: resnet101, Epoch: 40, Train Loss: 0.0266, Train Acc: 99.06, Test Loss: 0.0009, Test Acc: 83.65\n"
     ]
    }
   ],
   "source": [
    "for model_name, config in cifar_10_model_configs.items():\n",
    "    model = config['model'](pretrained=config['pretrained'], num_classes=config['num_classes']).to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_acc = train(model, device, train_loader, optimizer, criterion)\n",
    "        test_loss, test_acc = test(model, device, test_loader, criterion)\n",
    "        print(f\"Model: {model_name}, Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}\")\n",
    "        \n",
    "    model_path = f\"./models/{datasets_name}/{model_name}.pth\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d748dad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cifar10 dataset...\n",
      "Loading vgg16 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating original model...\n",
      "Original Model - Accuracy: 10.20%, Loss: 2.3202\n",
      "\n",
      "==================================================\n",
      "Testing beta = 1.0\n",
      "==================================================\n",
      "Applying neff-based pruning...\n",
      "\n",
      "Layer-wise pruning statistics (beta=1.0):\n",
      "  features.0: neff=920.7, kept=920/1728 (sparsity=46.8%)\n",
      "  features.2: neff=17836.2, kept=17836/36864 (sparsity=51.6%)\n",
      "  features.5: neff=38119.3, kept=38119/73728 (sparsity=48.3%)\n",
      "  features.7: neff=82056.5, kept=82056/147456 (sparsity=44.4%)\n",
      "  features.10: neff=166045.7, kept=166045/294912 (sparsity=43.7%)\n",
      "  features.12: neff=349219.0, kept=349219/589824 (sparsity=40.8%)\n",
      "  features.14: neff=342429.1, kept=342429/589824 (sparsity=41.9%)\n",
      "  features.17: neff=679219.2, kept=679219/1179648 (sparsity=42.4%)\n",
      "  features.19: neff=1421596.8, kept=1421596/2359296 (sparsity=39.7%)\n",
      "  features.21: neff=1420363.6, kept=1420363/2359296 (sparsity=39.8%)\n",
      "  features.24: neff=1438711.9, kept=1438711/2359296 (sparsity=39.0%)\n",
      "  features.26: neff=1467173.1, kept=1467173/2359296 (sparsity=37.8%)\n",
      "  features.28: neff=1423944.0, kept=1423944/2359296 (sparsity=39.6%)\n",
      "  classifier.0: neff=62945060.0, kept=62945060/102760448 (sparsity=38.7%)\n",
      "  classifier.3: neff=10598973.0, kept=10598973/16777216 (sparsity=36.8%)\n",
      "  classifier.6: neff=30736.7, kept=30736/40960 (sparsity=25.0%)\n",
      "Overall sparsity: 38.62%\n",
      "\n",
      "Pruned Model (before fine-tuning) - Accuracy: 11.01%, Loss: 2.7476\n",
      "\n",
      "Fine-tuning for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 391/391 [01:34<00:00,  4.13it/s, loss=0.00433, acc=81]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 80.97%, Test Acc: 89.53%, Test Loss: 0.3062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 391/391 [01:35<00:00,  4.11it/s, loss=0.00208, acc=91]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 90.97%, Test Acc: 90.68%, Test Loss: 0.2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 391/391 [01:33<00:00,  4.17it/s, loss=0.00142, acc=93.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 93.62%, Test Acc: 91.59%, Test Loss: 0.2448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 391/391 [01:33<00:00,  4.18it/s, loss=0.000986, acc=95.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 95.62%, Test Acc: 92.20%, Test Loss: 0.2384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 391/391 [01:34<00:00,  4.13it/s, loss=0.000789, acc=96.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 96.55%, Test Acc: 92.40%, Test Loss: 0.2336\n",
      "Best accuracy after fine-tuning: 92.40%\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT SUMMARY\n",
      "============================================================\n",
      "original: Acc=10.20%, Sparsity=0%\n",
      "beta_1.0: Before=11.01%, After=92.40%, Sparsity=38.6%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def mask_block(module: nn.Module, beta=1.0, method='magnitude') -> tuple:\n",
    "    \"\"\"Apply neff-based masking to a single module's weights.\"\"\"\n",
    "    x = module.weight.data\n",
    "    original_shape = x.shape\n",
    "    x = x.view(-1)\n",
    "    \n",
    "    if method == 'mean':\n",
    "        x = x - torch.mean(x)\n",
    "    \n",
    "    # L1 normalization\n",
    "    x_abs = torch.abs(x)\n",
    "    x_norm = x_abs / torch.sum(x_abs)\n",
    "    \n",
    "    # Calculate effective number of parameters\n",
    "    neff = 1 / torch.sum(x_norm ** 2)\n",
    "    \n",
    "    # Determine how many weights to keep\n",
    "    r_neff = torch.floor(beta * neff)\n",
    "    r_neff = r_neff.clamp(min=1, max=len(x)-1).long()\n",
    "    \n",
    "    # Sort by magnitude (using absolute values)\n",
    "    sorted_vals, indices = torch.sort(x_abs, descending=True)\n",
    "    \n",
    "    # Create mask for top r_neff weights\n",
    "    range_tensor = torch.arange(len(x), device=x.device)\n",
    "    sorted_mask = range_tensor < r_neff\n",
    "    \n",
    "    # Scatter back to original positions\n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    \n",
    "    # Reshape mask to match original weight shape\n",
    "    mask = mask.view(original_shape)\n",
    "    \n",
    "    return mask, neff.item()\n",
    "\n",
    "def model_block(model, renormalize=False, beta=1.0, method='magnitude'):\n",
    "    \"\"\"Apply neff-based pruning to entire model.\"\"\"\n",
    "    model = copy.deepcopy(model)\n",
    "    total_params = 0\n",
    "    pruned_params = 0\n",
    "    layer_stats = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Apply to both Linear and Conv2d layers\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            mask, neff = mask_block(module, beta=beta, method=method)\n",
    "            mask = mask.to(module.weight.device)\n",
    "            \n",
    "            # Count parameters\n",
    "            total_params += module.weight.numel()\n",
    "            pruned_params += (~mask).sum().item()\n",
    "            \n",
    "            # Store stats\n",
    "            layer_stats[name] = {\n",
    "                'neff': neff,\n",
    "                'total': module.weight.numel(),\n",
    "                'kept': mask.sum().item(),\n",
    "                'sparsity': 1 - (mask.sum().item() / module.weight.numel())\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if renormalize:\n",
    "                    # For Conv2d, sum over input channels (dim 1)\n",
    "                    # For Linear, sum over input features (dim 1)\n",
    "                    pre = module.weight.abs().sum(dim=1 if isinstance(module, nn.Conv2d) else 0, keepdim=True)\n",
    "                    module.weight *= mask\n",
    "                    post = module.weight.abs().sum(dim=1 if isinstance(module, nn.Conv2d) else 0, keepdim=True)\n",
    "                    # Avoid division by zero\n",
    "                    scale = torch.where(post > 0, pre / post, torch.ones_like(pre))\n",
    "                    module.weight *= scale\n",
    "                else:\n",
    "                    module.weight *= mask\n",
    "    \n",
    "    overall_sparsity = pruned_params / total_params if total_params > 0 else 0\n",
    "    return model, layer_stats, overall_sparsity\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model accuracy and loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += data.size(0)\n",
    "    \n",
    "    return total_loss / total, 100. * correct / total\n",
    "\n",
    "def fine_tune(model, train_loader, test_loader, epochs=10, lr=0.001, device='cuda'):\n",
    "    \"\"\"Fine-tune pruned model.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    model.to(device)\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += data.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'loss': train_loss/total, 'acc': 100.*correct/total})\n",
    "        \n",
    "        # Evaluation\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, '\n",
    "              f'Test Acc: {test_acc:.2f}%, Test Loss: {test_loss:.4f}')\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "def experiment_pruning(model_name='vgg16', dataset='cifar10', beta_values=[0.3, 0.5, 0.7, 1.0], \n",
    "                       renormalize=True, fine_tune_epochs=10, device='cuda'):\n",
    "    \"\"\"Run pruning experiments with different beta values.\"\"\"\n",
    "    \n",
    "    # Import dataloader function\n",
    "    from torchvision import datasets, transforms\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # Get data loaders\n",
    "    print(f\"Loading {dataset} dataset...\")\n",
    "    train_loader, test_loader, _, num_classes, _ = get_loaders(dataset)\n",
    "    \n",
    "    # Load pretrained model\n",
    "    print(f\"Loading {model_name} model...\")\n",
    "    if model_name == 'vgg16':\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        # Adjust final layer for dataset\n",
    "        model.classifier[-1] = nn.Linear(4096, num_classes)\n",
    "    elif model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        # Adjust final layer for dataset\n",
    "        model.fc = nn.Linear(2048, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Evaluate original model\n",
    "    print(\"Evaluating original model...\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    orig_loss, orig_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Original Model - Accuracy: {orig_acc:.2f}%, Loss: {orig_loss:.4f}\")\n",
    "    \n",
    "    results = {'original': {'accuracy': orig_acc, 'loss': orig_loss, 'sparsity': 0}}\n",
    "    \n",
    "    # Test different beta values\n",
    "    for beta in beta_values:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing beta = {beta}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Apply pruning\n",
    "        print(\"Applying neff-based pruning...\")\n",
    "        pruned_model, layer_stats, overall_sparsity = model_block(\n",
    "            model, renormalize=renormalize, beta=beta, method='magnitude'\n",
    "        )\n",
    "        \n",
    "        # Print layer statistics\n",
    "        print(f\"\\nLayer-wise pruning statistics (beta={beta}):\")\n",
    "        for name, stats in layer_stats.items():\n",
    "            print(f\"  {name}: neff={stats['neff']:.1f}, \"\n",
    "                  f\"kept={stats['kept']}/{stats['total']} \"\n",
    "                  f\"(sparsity={stats['sparsity']*100:.1f}%)\")\n",
    "        print(f\"Overall sparsity: {overall_sparsity*100:.2f}%\")\n",
    "        \n",
    "        # Evaluate pruned model before fine-tuning\n",
    "        pruned_loss, pruned_acc = evaluate(pruned_model, test_loader, criterion, device)\n",
    "        print(f\"\\nPruned Model (before fine-tuning) - Accuracy: {pruned_acc:.2f}%, Loss: {pruned_loss:.4f}\")\n",
    "        \n",
    "        # Fine-tune if specified\n",
    "        if fine_tune_epochs > 0:\n",
    "            print(f\"\\nFine-tuning for {fine_tune_epochs} epochs...\")\n",
    "            pruned_model, best_acc = fine_tune(\n",
    "                pruned_model, train_loader, test_loader, \n",
    "                epochs=fine_tune_epochs, lr=0.001, device=device\n",
    "            )\n",
    "            print(f\"Best accuracy after fine-tuning: {best_acc:.2f}%\")\n",
    "            \n",
    "            results[f'beta_{beta}'] = {\n",
    "                'accuracy_before': pruned_acc,\n",
    "                'accuracy_after': best_acc,\n",
    "                'sparsity': overall_sparsity * 100,\n",
    "                'layer_stats': layer_stats\n",
    "            }\n",
    "        else:\n",
    "            results[f'beta_{beta}'] = {\n",
    "                'accuracy': pruned_acc,\n",
    "                'loss': pruned_loss,\n",
    "                'sparsity': overall_sparsity * 100,\n",
    "                'layer_stats': layer_stats\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# DataLoader function (from your code)\n",
    "def get_loaders(dataset_name, batch_size=128, test_batch_size=1000, data_root='./data'):\n",
    "    \"\"\"Returns: train_loader, test_loader, input_size, num_classes, meta (dict)\"\"\"\n",
    "    from torchvision import datasets, transforms\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    name = dataset_name.lower()\n",
    "    meta = {}\n",
    "\n",
    "    # Generic normalizations\n",
    "    NORM_1C = transforms.Normalize((0.5,), (0.5,))\n",
    "    NORM_3C = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    if name == 'mnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train = datasets.MNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.MNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'fashionmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.FashionMNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.FashionMNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "        \n",
    "    elif name == 'cifar10':\n",
    "        tfm = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                NORM_3C\n",
    "            ])\n",
    "        train = datasets.CIFAR10(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR10(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 224*224*3, 10\n",
    "\n",
    "    elif name == 'cifar100':\n",
    "        tfm = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                NORM_3C\n",
    "            ])\n",
    "        train = datasets.CIFAR100(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR100(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 224*224*3, 100\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=test_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, test_loader, inp, ncls, meta\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Run experiments\n",
    "    results = experiment_pruning(\n",
    "        model_name='vgg16',  # or 'resnet50'\n",
    "        dataset='cifar10',\n",
    "        beta_values=[1.0],\n",
    "        renormalize=True,\n",
    "        fine_tune_epochs=5,  # Reduce for faster testing\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    for config, metrics in results.items():\n",
    "        if config == 'original':\n",
    "            print(f\"{config}: Acc={metrics['accuracy']:.2f}%, Sparsity=0%\")\n",
    "        else:\n",
    "            if 'accuracy_after' in metrics:\n",
    "                print(f\"{config}: Before={metrics['accuracy_before']:.2f}%, \"\n",
    "                      f\"After={metrics['accuracy_after']:.2f}%, \"\n",
    "                      f\"Sparsity={metrics['sparsity']:.1f}%\")\n",
    "            else:\n",
    "                print(f\"{config}: Acc={metrics['accuracy']:.2f}%, \"\n",
    "                      f\"Sparsity={metrics['sparsity']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25302c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
